# RAG Assistant Phase 1

## ğŸ¯ Project Overview
A modular Python-based Retrieval Augmented Generation (RAG) system for PDF document Q&A using open-source models and vector similarity search.

## ğŸš€ Features
- **PDF Text Extraction**: Extract and process text from PDF documents
- **Smart Text Chunking**: Split documents into semantically meaningful chunks
- **Vector Embeddings**: Generate embeddings using SentenceTransformers
- **Semantic Search**: Fast similarity search with FAISS vector store
- **Local LLM Integration**: Support for both Hugging Face Transformers and Ollama
- **Modular Architecture**: Clean, testable, and extensible codebase

## ğŸ—ï¸ Architecture

### Project Structure
```
rag-assistant-phase1/
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py          # Module initialization
â”‚   â”œâ”€â”€ pdf_parser.py        # PDF text extraction
â”‚   â”œâ”€â”€ text_splitter.py     # Text chunking logic
â”‚   â”œâ”€â”€ embedder.py          # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py      # FAISS vector storage
â”‚   â””â”€â”€ rag_pipeline.py      # Main RAG orchestration
â”œâ”€â”€ tests/                   # Unit tests
â”œâ”€â”€ sample_data/            # Sample PDFs and test data
â”œâ”€â”€ main.py                 # CLI application entry point
â”œâ”€â”€ config.py               # Configuration settings
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md              # This file
```

### Data Flow
1. **PDF Upload** â†’ Extract text using PyPDF2
2. **Text Processing** â†’ Split into overlapping chunks
3. **Embedding Generation** â†’ Convert chunks to vectors using SentenceTransformers
4. **Vector Storage** â†’ Index vectors in FAISS for fast retrieval
5. **Query Processing** â†’ Find relevant chunks via similarity search
6. **Answer Generation** â†’ Use local LLM with retrieved context

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8+ (no specific version required)
- 8GB+ RAM recommended for local LLM inference
- Git for version control

### Setup Instructions

1. **Clone and navigate to project**:
```bash
git clone <repository-url>
cd rag-assistant-phase1
```

2. **Create virtual environment**:
```bash
python -m venv .venv
# Windows
.venv\Scripts\activate
# Linux/Mac
source .venv/bin/activate
```

3. **Install dependencies**:
```bash
pip install -r requirements.txt
```

4. **Verify installation**:
```bash
python main.py --help
```

## ğŸ® Usage

### Basic Usage
```bash
# Process a PDF and ask questions
python main.py --pdf "sample_data/sample_document.pdf" --query "What is the main topic?"

# Interactive mode
python main.py --interactive
```

### Python API Usage
```python
from modules.rag_pipeline import RAGPipeline

# Initialize pipeline
rag = RAGPipeline()

# Process document
rag.load_document("path/to/document.pdf")

# Ask questions
answer = rag.query("What are the key findings?")
print(answer)
```

## ğŸ”§ Configuration

### LLM Options
The system supports two local LLM backends:

**Option 1: Hugging Face Transformers**
```python
# config.py
LLM_BACKEND = "huggingface"
HF_MODEL_NAME = "microsoft/DialoGPT-medium"  # Lightweight option
# HF_MODEL_NAME = "meta-llama/Llama-2-7b-chat-hf"  # More powerful
```

**Option 2: Ollama**
```python
# config.py
LLM_BACKEND = "ollama"
OLLAMA_MODEL = "llama2:7b"
```

### Embedding Configuration
```python
# config.py
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
TOP_K_RETRIEVAL = 5
```

## ğŸ§ª Testing

### Run All Tests
```bash
pytest tests/ -v
```

### Run with Coverage
```bash
pytest tests/ --cov=modules --cov-report=html
```

### Manual Testing
```bash
# Test individual modules
python -m modules.pdf_parser sample_data/sample_document.pdf
python -m modules.embedder --test
```

## ğŸ“Š Performance

### Benchmarks
- **PDF Processing**: ~2-5 seconds for typical documents (10-50 pages)
- **Embedding Generation**: ~1-3 seconds per 1000 chunks
- **Query Response**: ~2-8 seconds (depending on LLM size)
- **Memory Usage**: 2-8GB (varies by LLM model)

### Optimization Tips
- Use smaller LLM models for faster inference
- Increase `CHUNK_SIZE` for fewer, longer contexts
- Adjust `TOP_K_RETRIEVAL` based on document complexity

## ğŸ› Troubleshooting

### Common Issues

**1. Memory Errors**
```bash
# Solution: Use smaller models or increase system RAM
# In config.py, try:
HF_MODEL_NAME = "distilgpt2"  # Lightweight alternative
```

**2. Slow Performance**
```bash
# Solution: Reduce model size or chunk count
CHUNK_SIZE = 800  # Larger chunks = fewer embeddings
TOP_K_RETRIEVAL = 3  # Fewer context chunks
```

**3. Poor Answer Quality**
```bash
# Solution: Adjust retrieval parameters
CHUNK_OVERLAP = 100  # More context overlap
TOP_K_RETRIEVAL = 7  # More relevant context
```

## ğŸ”„ Development Workflow

### Adding New Features
1. Create feature branch: `git checkout -b feature/new-feature`
2. Implement in appropriate module
3. Add comprehensive tests in `tests/`
4. Update documentation
5. Submit pull request

### Module Development
- Follow existing code patterns
- Include docstrings with examples
- Add type hints where applicable
- Write unit tests for new functions

## ğŸ¯ Phase 1 Completion Checklist
- [x] Project structure and environment
- [x] PDF text extraction (`pdf_parser.py`)
- [x] Text chunking system (`text_splitter.py`)
- [x] Embedding generation (`embedder.py`)
- [x] Vector storage with FAISS (`vector_store.py`)
- [x] LLM integration (both HF and Ollama)
- [x] End-to-end RAG pipeline (`rag_pipeline.py`)
- [x] CLI interface (`main.py`)
- [x] Error handling and logging
- [x] Documentation and examples

## ğŸš€ Next Steps (Future Phases)
- **Phase 2**: Multilingual support and speech integration
- **Phase 3**: Web interface with Streamlit
- **Phase 4**: Advanced features (memory, chat history)

## ğŸ¤ Contributing
1. Fork the repository
2. Create feature branch
3. Make changes with tests
4. Submit pull request with clear description

## ğŸ“ License
This project is licensed under the MIT License - see the LICENSE file for details.

---

**Built with â¤ï¸ for efficient, local RAG implementations**
